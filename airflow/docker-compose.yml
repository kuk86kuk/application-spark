version: '3.8'
networks:
  my-global-network:
    name: my-global-network
    driver: bridge
    external: true

services:
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - my-global-network

  redis:
    image: redis:latest
    networks:
      - my-global-network

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    restart: unless-stopped
    depends_on:
      - postgres
      - redis
    environment:
      - AIRFLOW__WEBSERVER__SECRET_KEY=your_secret_key_here
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow
      - JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
      - HADOOP_CONF_DIR=/opt/hadoop-conf
    ports:
      - "8080:8080"
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      bash -c "
      pip install apache-airflow-providers-apache-spark==4.1.0 &&
      while ! airflow db check; do sleep 5; done &&
      airflow webserver
      "
    networks:
      - my-global-network

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    restart: unless-stopped
    depends_on:
      - postgres
      - redis
    environment:
      - AIRFLOW__WEBSERVER__SECRET_KEY=your_secret_key_here
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow
      - JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
      - HADOOP_CONF_DIR=/opt/hadoop-conf
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      bash -c "
      pip install apache-airflow-providers-apache-spark==4.1.0 &&
      while ! airflow db check; do sleep 5; done &&
      airflow scheduler
      "
    networks:
      - my-global-network

  airflow-worker:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    restart: unless-stopped
    depends_on:
      - postgres
      - redis
    environment:
      - AIRFLOW__WEBSERVER__SECRET_KEY=your_secret_key_here
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow
      - JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
      - HADOOP_CONF_DIR=/opt/hadoop-conf
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      bash -c "
      pip install apache-airflow-providers-apache-spark==4.1.0 &&
      while ! airflow db check; do sleep 5; done &&
      airflow celery worker
      "
    networks:
      - my-global-network

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    depends_on:
      - postgres
      - redis
    environment:
      - AIRFLOW__WEBSERVER__SECRET_KEY=your_secret_key_here
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postcss://airflow:airflow@postgres:5432/airflow
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      bash -c "
      pip install apache-airflow-providers-apache-spark==4.1.0 &&
      airflow db init &&
      airflow users create \
        --username admin \
        --firstname Admin \
        --lastname Admin \
        --role Admin \
        --email admin@example.com \
        --password admin
      "
    networks:
      - my-global-network

volumes:
  postgres_data:
  airflow_dags:
    external: true
  airflow_logs:
  airflow_plugins: