[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO SparkContext: Running Spark version 3.4.4
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO ResourceUtils: ==============================================================
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO ResourceUtils: ==============================================================
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO SparkContext: Submitted application: MySparkApp_stage_preload
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO SecurityManager: Changing view acls to: ***
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO SecurityManager: Changing modify acls to: ***
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO SecurityManager: Changing view acls groups to:
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO SecurityManager: Changing modify acls groups to:
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO Utils: Successfully started service 'sparkDriver' on port 46665.
[2025-05-13, 09:09:15 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:15 INFO SparkEnv: Registering MapOutputTracker
[2025-05-13, 09:09:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:16 INFO SparkEnv: Registering BlockManagerMaster
[2025-05-13, 09:09:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-05-13, 09:09:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-05-13, 09:09:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-05-13, 09:09:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-382d32d6-36d6-4edb-8a55-8016cdde3c64
[2025-05-13, 09:09:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:16 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-05-13, 09:09:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:16 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-05-13, 09:09:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:16 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-05-13, 09:09:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-05-13, 09:09:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:16 INFO SparkContext: Added file hdfs://namenode:8020/user/jenkins/application_spark/dependencies.zip at hdfs://namenode:8020/user/jenkins/application_spark/dependencies.zip with timestamp 1747127355509
[2025-05-13, 09:09:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:16 INFO Utils: Fetching hdfs://namenode:8020/user/jenkins/application_spark/dependencies.zip to /tmp/spark-07d98b04-0819-4361-a405-2051c63559df/userFiles-556146f3-2f6b-4e11-9b11-2c2efffcfd41/fetchFileTemp14016518333826744081.tmp
[2025-05-13, 09:09:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-05-13, 09:09:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:16 INFO TransportClientFactory: Successfully created connection to spark-master/172.24.0.2:7077 after 27 ms (0 ms spent in bootstraps)
[2025-05-13, 09:09:36 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-05-13, 09:09:56 UTC] {spark_submit.py:490} INFO - 25/05/13 09:09:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-05-13, 09:10:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:16 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.
[2025-05-13, 09:10:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:16 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.
[2025-05-13, 09:10:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44581.
[2025-05-13, 09:10:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:16 INFO NettyBlockTransferService: Server created on 767af9a0bacd:44581
[2025-05-13, 09:10:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-05-13, 09:10:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 767af9a0bacd, 44581, None)
[2025-05-13, 09:10:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:16 INFO BlockManagerMasterEndpoint: Registering block manager 767af9a0bacd:44581 with 434.4 MiB RAM, BlockManagerId(driver, 767af9a0bacd, 44581, None)
[2025-05-13, 09:10:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 767af9a0bacd, 44581, None)
[2025-05-13, 09:10:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 767af9a0bacd, 44581, None)
[2025-05-13, 09:10:16 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:16 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - warnings.warn("Python 3.7 support is deprecated in Spark 3.4.", FutureWarning)
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - ============================================================
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - _____  _    _ ______ _____
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - |  __ \| |  | |  ____|  __ \
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - | |  | | |  | | |__  | |__) |
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - | |  | | |  | |  __| |  ___/
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - | |__| | |__| | |____| |
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - |_____/ \____/|______|_|
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - >>> PRELOAD <<<
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - Environment: prod
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - Timestamp: 2025-05-13 09:10:17
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - ============================================================
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:17 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:17 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:17 INFO SparkUI: Stopped Spark web UI at http://767af9a0bacd:4040
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:17 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:17 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:17 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:17 INFO MemoryStore: MemoryStore cleared
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:17 INFO BlockManager: BlockManager stopped
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:17 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-05-13, 09:10:17 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:17 INFO SparkContext: Successfully stopped SparkContext
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - preload
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - hdfs://namenode:8020/datamarts/DataMart_transaction/ddl/schema.sql
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - Error in task stage_preload: An error occurred while calling o44.defaultParallelism.
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - : java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - This stopped SparkContext was created at:
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.Gateway.invoke(Gateway.java:238)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - java.base/java.lang.Thread.run(Thread.java:829)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - The currently active SparkContext was created at:
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - (No active SparkContext.)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2554)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:18 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:18 INFO SparkContext: SparkContext already stopped.
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - --------------------------------------------------
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - EXECUTION TIME: 1.23 sec
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - STATUS: COMPLETED
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - --------------------------------------------------
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - Traceback (most recent call last):
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - File "/tmp/spark-09f6465b-51a2-4aeb-8317-ded253e5ab5c/app.py", line 71, in <module>
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - main()
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - File "/tmp/spark-09f6465b-51a2-4aeb-8317-ded253e5ab5c/app.py", line 27, in main
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - file_content = spark.sparkContext.textFile(hdfs_file_path).collect()
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - File "/home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py", line 986, in textFile
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - File "/home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py", line 627, in defaultParallelism
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - File "/home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1323, in __call__
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - File "/home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - File "/home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 328, in get_return_value
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o44.defaultParallelism.
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - : java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - This stopped SparkContext was created at:
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.Gateway.invoke(Gateway.java:238)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - java.base/java.lang.Thread.run(Thread.java:829)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - The currently active SparkContext was created at:
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - (No active SparkContext.)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2554)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:18 INFO ShutdownHookManager: Shutdown hook called
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-07d98b04-0819-4361-a405-2051c63559df
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-07d98b04-0819-4361-a405-2051c63559df/pyspark-633dc1f0-c749-4c14-a7a7-c31d36d4cc30
[2025-05-13, 09:10:18 UTC] {spark_submit.py:490} INFO - 25/05/13 09:10:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-09f6465b-51a2-4aeb-8317-ded253e5ab5c
[2025-05-13, 09:10:18 UTC] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 422, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.hadoop.fs.defaultFS=hdfs://namenode:8020 --conf spark.hadoop.dfs.client.use.datanode.hostname=true --conf spark.hadoop.dfs.replication=1 --conf spark.hadoop.ipc.max.response.size=104857600 --conf spark.submit.deployMode=client --conf spark.driver.memory=1g --conf spark.executor.memory=1g --conf spark.executor.instances=1 --conf spark.executor.cores=1 --conf spark.network.timeout=600s --conf spark.sql.shuffle.partitions=100 --py-files hdfs://namenode:8020/user/jenkins/application_spark/dependencies.zip --name arrow-spark --verbose hdfs://namenode:8020/user/jenkins/application_spark/app.py --env prod --step preload --task-id stage_preload --query_mapping source:transactions_stg --datamart transactions --query_path hdfs://namenode:8020/queries/preload.sql --table_schema stg --table_name transactions_preload --repartition 40 --partition_by load_date --bucket_by _ --num_buckets 300 --location _ --do_truncate_table y --do_drop_table n --do_msck_repair_table n --temp_view_name _ --cache_df n. Error code is: 1.
[2025-05-13, 09:10:18 UTC] {taskinstance.py:1350} INFO - Marking task as UP_FOR_RETRY. dag_id=spark_full_data_pipeline, task_id=spark_stage_preload, execution_date=20250513T090908, start_date=20250513T090910, end_date=20250513T091018
[2025-05-13, 09:10:18 UTC] {standard_task_runner.py:109} ERROR - Failed to execute job 266 for task spark_stage_preload (Cannot execute: spark-submit --master spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.hadoop.fs.defaultFS=hdfs://namenode:8020 --conf spark.hadoop.dfs.client.use.datanode.hostname=true --conf spark.hadoop.dfs.replication=1 --conf spark.hadoop.ipc.max.response.size=104857600 --conf spark.submit.deployMode=client --conf spark.driver.memory=1g --conf spark.executor.memory=1g --conf spark.executor.instances=1 --conf spark.executor.cores=1 --conf spark.network.timeout=600s --conf spark.sql.shuffle.partitions=100 --py-files hdfs://namenode:8020/user/jenkins/application_spark/dependencies.zip --name arrow-spark --verbose hdfs://namenode:8020/user/jenkins/application_spark/app.py --env prod --step preload --task-id stage_preload --query_mapping source:transactions_stg --datamart transactions --query_path hdfs://namenode:8020/queries/preload.sql --table_schema stg --table_name transactions_preload --repartition 40 --partition_by load_date --bucket_by _ --num_buckets 300 --location _ --do_truncate_table y --do_drop_table n --do_msck_repair_table n --temp_view_name _ --cache_df n. Error code is: 1.; 3504)
[2025-05-13, 09:10:18 UTC] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2025-05-13, 09:10:18 UTC] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check