pipeline {
    agent any
    
    parameters {
        string(
            name: 'GIT_REPO_URL',
            defaultValue: 'https://github.com/kuk86kuk/application_spark',
            description: 'URL Git-репозитория с конфигами'
        )
        string(
            name: 'HDFS_BASE_PATH',
            defaultValue: '/user/jenkins/application_spark',
            description: 'Базовый путь в HDFS для загрузки приложений'
        )
        string(
            name: 'WEBHDFS_NAMENODE',
            defaultValue: 'http://namenode:9870/webhdfs/v1',
            description: 'URL WebHDFS Namenode'
        )
        string(
            name: 'HDFS_USER',
            defaultValue: 'jenkins',
            description: 'Пользователь HDFS'
        )
        string(
            name: 'SPARK_APP_DIR',
            defaultValue: 'application_spark',
            description: 'Имя папки с Spark-приложением'
        )
    }

    environment {
        HDFS_BASE_PATH = "${params.HDFS_BASE_PATH}"
        WEBHDFS_NAMENODE = "${params.WEBHDFS_NAMENODE}"
        HDFS_USER = "${params.HDFS_USER}"
        SPARK_APP_DIR = "${params.SPARK_APP_DIR}"
        ZIP_FILE = "dependencies.zip"
    }

    stages {
        stage('Подготовка окружения') {
            steps {
                script {
                    if (isUnix()) {
                        def zipInstalled = sh(script: 'command -v zip || echo "not installed"', returnStdout: true).trim()
                        if (zipInstalled == "not installed") {
                            echo "Установка zip..."
                            try {
                                sh """
                                    apt-get update -qq && \
                                    apt-get install -y --no-install-recommends zip
                                """
                            } catch (Exception e) {
                                echo "⚠️ Не удалось установить zip, попробуем использовать Python"
                                env.USE_PYTHON_ZIP = "true"
                            }
                        }
                    }
                }
            }
        }
        
        stage('Получение кода') {
            steps {
                checkout([
                    $class: 'GitSCM',
                    branches: [[name: '*/main']],
                    extensions: [],
                    userRemoteConfigs: [[
                        url: 'https://github.com/kuk86kuk/application-spark.git',
                        branch: 'main'
                    ]]
                ])
            }
        }
        
        stage('Проверка структуры') {
            steps {
                script {
                    dir(env.SPARK_APP_DIR) {
                        if (!fileExists('app.py')) {
                            error "❌ Основной файл app.py не найден"
                        }
                        
                        def requiredDirs = ['steps', 'checks', 'config', 'utils']
                        def missingDirs = requiredDirs.findAll { !fileExists(it) }
                        if (missingDirs) {
                            error "❌ Отсутствуют обязательные директории: ${missingDirs.join(', ')}"
                        }
                    }
                }
            }
        }
        
        stage('Создание архива') {
            steps {
                script {
                    if (env.USE_PYTHON_ZIP == "true") {
                        echo "Создание архива с помощью Python..."
                        sh """
                            python3 -c "
import os
import zipfile
with zipfile.ZipFile('${env.ZIP_FILE}', 'w') as zipf:
    for folder in ['steps', 'checks', 'config', 'utils']:
        for root, dirs, files in os.walk(os.path.join('${env.SPARK_APP_DIR}', folder)):
            for file in files:
                zipf.write(os.path.join(root, file), 
                          os.path.relpath(os.path.join(root, file), 
                                        os.path.join('${env.SPARK_APP_DIR}', '..')))
                            "
                        """
                    } else {
                        echo "Создание архива с помощью zip..."
                        sh """
                            cd ${env.SPARK_APP_DIR} && \
                            zip -r "../${env.ZIP_FILE}" steps checks config utils && \
                            cd ..
                        """
                    }
                    
                    if (!fileExists(env.ZIP_FILE)) {
                        error "❌ Не удалось создать архив зависимостей"
                    }
                    echo "Размер архива: ${sh(script: "du -h ${env.ZIP_FILE} | cut -f1", returnStdout: true).trim()}"
                }
            }
        }
        
        stage('Загрузка в HDFS') {
            steps {
                script {
                    def hdfsAppPath = "${env.HDFS_BASE_PATH}"
                    
                    echo "Создание HDFS-директории: ${hdfsAppPath}"
                    hdfsCreateDirectory(hdfsAppPath)
                    
                    echo "Удаление старых версий файлов из HDFS..."
                    // Удаляем архив
                    deleteFromHDFS("${hdfsAppPath}/${env.ZIP_FILE}")
                    
                    // Удаляем app.py
                    deleteFromHDFS("${hdfsAppPath}/app.py")
                    
                    echo "Загрузка архива в HDFS..."
                    uploadToHDFS(env.ZIP_FILE, "${hdfsAppPath}/${env.ZIP_FILE}")
                    
                    echo "Загрузка app.py в HDFS..."
                    uploadToHDFS("${env.SPARK_APP_DIR}/app.py", "${hdfsAppPath}/app.py")
                    
                    env.HDFS_APP_PATH = hdfsAppPath
                }
            }
        }
    }
    
    post {
        success {
            script {
                def namenodeHost = sh(
                    script: "echo '${env.WEBHDFS_NAMENODE}' | sed -e 's|^http://||' -e 's|/.*||'",
                    returnStdout: true
                ).trim()
                
                echo """
                ✅ Успешно!
                Основной файл: hdfs://${namenodeHost}${env.HDFS_APP_PATH}/app.py
                Архив зависимостей: hdfs://${namenodeHost}${env.HDFS_APP_PATH}/${env.ZIP_FILE}
                
                Пример команды для запуска:
                spark-submit \\
                  --master spark://spark-master:7077 \\
                  --py-files hdfs://${namenodeHost}${env.HDFS_APP_PATH}/${env.ZIP_FILE} \\
                  hdfs://${namenodeHost}${env.HDFS_APP_PATH}/app.py
                """
            }
        }
        failure {
            echo "❌ Сборка завершилась с ошибкой"
        }
        always {
            cleanWs()
        }
    }
}

// Вспомогательные функции
def hdfsCreateDirectory(String path) {
    sh """
        curl -s -X PUT "${env.WEBHDFS_NAMENODE}${path}?op=MKDIRS&user.name=${env.HDFS_USER}"
    """
}

def uploadToHDFS(String localFile, String hdfsPath) {
    sh """
        redirect_url=\$(curl -s -i -X PUT \
          "${env.WEBHDFS_NAMENODE}${hdfsPath}?op=CREATE&user.name=${env.HDFS_USER}&overwrite=true" \
          | grep -i '^Location:' | awk '{print \$2}' | tr -d '\r\n')
        
        if [ -z "\$redirect_url" ]; then
          echo "❌ Не удалось получить URL для загрузки"
          exit 1
        fi
        
        if ! curl -s -X PUT -T "${localFile}" "\$redirect_url"; then
          echo "❌ Ошибка загрузки файла ${localFile}"
          exit 1
        fi
        
        echo "✓ Успешно загружено: ${localFile} -> ${hdfsPath}"
    """
}

def deleteFromHDFS(String hdfsPath) {
    sh """
        # Удаление файла через WebHDFS API
        if ! curl -s -i -X DELETE \
          "${env.WEBHDFS_NAMENODE}${hdfsPath}?op=DELETE&user.name=${env.HDFS_USER}&recursive=true" \
          | grep -q 'HTTP/.*200'; then
          
          echo "⚠️ Файл не существует или ошибка при удалении: ${hdfsPath}"
          # Не выходим с ошибкой, так как возможно файл уже удален
        else
          echo "✓ Успешно удалено: ${hdfsPath}"
        fi
    """
}