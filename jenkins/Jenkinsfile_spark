pipeline {
    agent any
    
    parameters {
        string(
            name: 'GIT_REPO_URL',
            defaultValue: 'https://github.com/kuk86kuk/application-spark',
            description: 'URL Git-репозитория с конфигами'
        )
        string(
            name: 'HDFS_BASE_PATH',
            defaultValue: '/user/jenkins/application_spark',
            description: 'Базовый путь в HDFS для загрузки приложений'
        )
        string(
            name: 'WEBHDFS_NAMENODE',
            defaultValue: 'http://namenode:9870/webhdfs/v1',
            description: 'URL WebHDFS Namenode'
        )
        string(
            name: 'HDFS_USER',
            defaultValue: 'jenkins',
            description: 'Пользователь HDFS'
        )
        string(
            name: 'SPARK_APP_DIR',
            defaultValue: 'application_spark',
            description: 'Имя папки с Spark-приложением'
        )
    }

    environment {
        HDFS_BASE_PATH = "${params.HDFS_BASE_PATH}"
        WEBHDFS_NAMENODE = "${params.WEBHDFS_NAMENODE}"
        HDFS_USER = "${params.HDFS_USER}"
        SPARK_APP_DIR = "${params.SPARK_APP_DIR}"
    }

    stages {
        stage('Подготовка') {
            steps {
                script {
                    // Проверка обязательных параметров
                    def requiredParams = [
                        'GIT_REPO_URL',
                        'HDFS_BASE_PATH',
                        'WEBHDFS_NAMENODE',
                        'HDFS_USER',
                        'SPARK_APP_DIR'
                    ]
                    
                    requiredParams.each { param ->
                        if (!params[param]?.trim()) {
                            error "❌ Не задан обязательный параметр: ${param}"
                        }
                    }
                    
                    echo "Параметры конфигурации:"
                    echo "Git Repo URL: ${params.GIT_REPO_URL}"
                    echo "HDFS Base Path: ${params.HDFS_BASE_PATH}"
                    echo "WebHDFS Namenode: ${params.WEBHDFS_NAMENODE}"
                    echo "HDFS User: ${params.HDFS_USER}"
                    echo "Spark App Dir: ${params.SPARK_APP_DIR}"
                }
            }
        }
        
        stage('Получение кода') {
            steps {
                checkout([
                    $class: 'GitSCM',
                    branches: [[name: '*/main']],
                    extensions: [],
                    userRemoteConfigs: [[
                        url: "${params.GIT_REPO_URL}",
                        credentialsId: 'github-ssh-key'
                    ]]
                ])
                sh 'ls -al'
            }
        }
        
        stage('Проверка структуры') {
            steps {
                script {
                    if (!fileExists(env.SPARK_APP_DIR)) {
                        error "❌ Папка с Spark-приложением '${env.SPARK_APP_DIR}' не найдена"
                    }
                    
                    def fileCount = sh(script: "find ${env.SPARK_APP_DIR} -type f | wc -l", returnStdout: true).trim()
                    if (fileCount == "0") {
                        error "❌ В папке ${env.SPARK_APP_DIR} не найдены файлы"
                    }
                    
                    echo "Найдена папка с Spark-приложением: ${env.SPARK_APP_DIR}"
                    echo "Количество файлов: ${fileCount}"
                }
            }
        }
        
        stage('Перенос в HDFS') {
            steps {
                script {
                    def fullHdfsPath = "${env.HDFS_BASE_PATH}"
                    
                    echo "Создание HDFS директории: ${fullHdfsPath}"
                    hdfsDeleteRecursive(fullHdfsPath)
                    hdfsCreateDirectory(fullHdfsPath)
                    
                    // Используем более простой подход для загрузки файлов
                    sh """
                        find ${env.SPARK_APP_DIR} -type f | while read file; do
                            relative_path="\${file#${env.SPARK_APP_DIR}/}"
                            hdfs_path="${fullHdfsPath}/\${relative_path}"
                            hdfs_dir="\$(dirname "\$hdfs_path")"
                            
                            # Создаем директорию в HDFS
                            curl -s -X PUT "${env.WEBHDFS_NAMENODE}\${hdfs_dir}?op=MKDIRS&user.name=${env.HDFS_USER}"
                            
                            # Загружаем файл
                            redirect_url=\$(curl -s -i -X PUT "${env.WEBHDFS_NAMENODE}\${hdfs_path}?op=CREATE&user.name=${env.HDFS_USER}&overwrite=true" | grep -i '^Location:' | awk '{print \$2}' | tr -d '\r\n')
                            curl -s -X PUT -T "\$file" "\$redirect_url" && \
                            echo "Uploaded: \$file -> \$hdfs_path" || \
                            echo "Failed to upload: \$file"
                        done
                    """
                    
                    env.HDFS_APP_PATH = fullHdfsPath
                }
            }
        }
    }
    
    post {
        success {
            script {
                def namenodeHost = sh(
                    script: "echo '${env.WEBHDFS_NAMENODE}' | sed -e 's|^http://||' -e 's|/.*||'",
                    returnStdout: true
                ).trim()
                
                echo "✅ Структура успешно перенесена в HDFS"
                echo "Путь: hdfs://${namenodeHost}${env.HDFS_APP_PATH}"
                echo "Для доступа используйте:"
                echo "hdfs dfs -ls ${env.HDFS_APP_PATH}"
            }
        }
        failure {
            echo "❌ Ошибка при переносе структуры"
            script {
                currentBuild.result = 'FAILURE'
            }
        }
        cleanup {
            echo "Очистка рабочих файлов..."
            cleanWs()
        }
    }
}

// ===== Вспомогательные функции =====
def hdfsCreateDirectory(String path) {
    sh """
        curl -f -s -X PUT \
        "${env.WEBHDFS_NAMENODE}${path}?op=MKDIRS&user.name=${env.HDFS_USER}"
    """
}

def hdfsDeleteRecursive(String path) {
    sh """
        curl -f -s -X DELETE \
        "${env.WEBHDFS_NAMENODE}${path}?op=DELETE&recursive=true&user.name=${env.HDFS_USER}" || true
    """
}