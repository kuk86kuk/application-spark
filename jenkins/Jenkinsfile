pipeline {
    agent any
    
    parameters {
        string(
            name: 'GIT_REPO_URL',
            defaultValue: 'https://github.com/kuk86kuk/DataMart_transaction',
            description: 'URL Git-репозитория с конфигами'
        )
        string(
            name: 'CONFIG_DIR',
            defaultValue: './wf',
            description: 'Путь к директории с YAML-конфигами'
        )
        string(
            name: 'DATAMARTS',
            defaultValue: 'DataMart_transaction1',
            description: 'Название витрины'
        )
    }

    environment {
        HADOOP_NAMENODE = "exec"
        AIRFLOW_DAGS_DIR = "/opt/airflow/dags"
        HDFS_SQL_DIR = "/datamarts/${params.DATAMARTS}"
        REPO_NAME = "${env.JOB_NAME}"

        AIRFLOW_API = "http://airflow-airflow-1:8080/api/v1"
        AIRFLOW_USER = "admin"
        AIRFLOW_PASS = "admin"
    }

    stages {
        stage('Получение кода') {
            steps {
                checkout([
                    $class: 'GitSCM',
                    branches: [[name: '*/main']],
                    extensions: [],
                    userRemoteConfigs: [[
                        url: "${params.GIT_REPO_URL}",
                        credentialsId: 'github-ssh-key'
                        
                    ]]
                ])
                sh 'ls -R'
            }
        }

       stage('Чтение YAML конфигов и генерация DAG') {
    steps {
        script {
            echo "=== НАЧАЛО ОБРАБОТКИ YAML КОНФИГОВ ==="
            
            // 1. Обработка common.yaml
            echo "Поиск common.yaml в ${params.CONFIG_DIR}"
            def commonConfig = [:]
            try {
                commonConfig = readYaml file: "${params.CONFIG_DIR}/common.yaml"
                echo "Успешно прочитан common.yaml:"
                echo "Версия: ${commonConfig.version ?: 'не указана'}"
                echo "Другие параметры: ${commonConfig.findAll { it.key != 'version' }}"
            } catch (Exception e) {
                echo "⚠️ ВНИМАНИЕ: Не удалось прочитать common.yaml"
                echo "Ошибка: ${e.getMessage()}"
                echo "Продолжаем работу без common.yaml"
            }

            // 2. Поиск других YAML-файлов
            echo "Поиск других YAML-файлов в ${params.CONFIG_DIR}"
            def findCmd = "find ${params.CONFIG_DIR} -type f \\( -name '*.yaml' -o -name '*.yml' \\) ! -name 'common.yaml' 2>/dev/null || echo ''"
            echo "Выполняем команду: ${findCmd}"
            
            def otherFiles = sh(script: findCmd, returnStdout: true).trim().split('\n')
            echo "Найдено файлов: ${otherFiles.size()}"
            
            def allConfigs = [:]
            otherFiles.eachWithIndex { filePath, index ->
                echo "\n=== Обработка файла ${index + 1}/${otherFiles.size()}: ${filePath} ==="
                // Чтение файла
                def config = readYaml file: filePath
                echo "Файл прочитан успешно"
                
                // Объединение с commonConfig
                def mergedConfig = commonConfig + config
                def configName = filePath.tokenize('/').last().replace('.yaml', '').replace('.yml', '')
                allConfigs[configName] = mergedConfig
            }

            echo "\n=== ИТОГИ ОБРАБОТКИ КОНФИГОВ ==="
            echo "Всего успешно обработано конфигов: ${allConfigs.size()}"
            echo "Список конфигов: ${allConfigs.keySet().join(', ')}"

            echo "=== НАЧАЛО ГЕНЕРАЦИИ DAG ФАЙЛОВ ==="
            
           
            // Process each config to create a DAG
            allConfigs.each { configName, config ->
                echo "${config}"
                def dagId = "${params.DATAMARTS}_${configName}"
                def schedule_interval = config.pipeline.schedule_interval ?: false
                def startDate = config.pipeline.startDate ?: "datetime(2025, 1, 1)"

                def application = config.application ?: false
                def conn_id = config.conn_id ?: false
                def application_args = config.application_args 

                echo "Создание DAG: ${dagId}"
                def expectedStages = [
                    "stage_preload",
                    "stage_calc_stg",
                    "stage_check_stg",
                    "stage_calc_inc",
                    "stage_check_inc",
                    "stage_MTP",
                    "stage_hist",
                    "final_check"
                ]
                 // Формируем задачи для DAG
                def tasksContent = new StringBuilder()
                def dependencies = []
                def previousTask = "start_task"
                
                expectedStages.each { stageName ->
                    def stageConfig = config.pipeline.stages?.get(stageName)
                    echo "stageConfig ${stageConfig}"
                 
                    if (stageConfig) {
        echo "✅ Stage '${stageName}' найден в конфиге: ${stageConfig}"
        
        // Преобразуем параметры в нужный формат
        def params = stageConfig.params ?: [:]
        def sparkArgs = params.collect { key, value ->
            "${key}='${value}'"
        }.join(",\n    ")
        def task_id = "${dagId}" + "${stageName}"
        tasksContent.append("""
${stageName} = create_spark_task(
    task_id="${task_id}",
    ${sparkArgs}
    )
""")
                        // Добавляем зависимость
                dependencies << "${previousTask} >> ${stageName}"
                previousTask = stageName
                }
                
                }
                // Финализируем зависимости
                def dependenciesContent = dependencies.join("\n") + " >> end_task"

                // Generate DAG content
                def dagContent = """from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    '${dagId}',
    default_args=default_args,
    description='Spark pipeline with Hadoop using BashOperator',
    schedule_interval=None,
    start_date=datetime(2025, 5, 20),
    catchup=False,
    tags=['spark', 'prod'],
    max_active_runs=1,
)

def create_spark_task(
    task_id: str,
    query_mapping: str = "source:default",
    datamart: str = "default",
    query_path: str = "hdfs://namenode:8020/default/query.sql",
    table_schema: str = "default",
    table_name: str = "default_table",
    repartition: str = "40",
    partition_by: str = "none",
    bucket_by: str = "none",
    num_buckets: str = "1",
    location: str = "hdfs://namenode:8020/user/jenkins/data",
    do_truncate_table: str = "false",
    do_drop_table: str = "false",
    do_msck_repair_table: str = "false",
    temp_view_name: str = "default_view",
    cache_df: str = "false"
):
    
    bash_command = f\"\"\"
    export HADOOP_CONF_DIR=/opt/hadoop-conf && \
    docker exec hadoop-namenode-1 /opt/spark-bin/bin/spark-submit \
      --master yarn \
      --deploy-mode client \
      --conf spark.pyspark.python=/usr/bin/python3.8 \
      --conf spark.yarn.resourcemanager.hostname=resourcemanager \
      --conf spark.yarn.resourcemanager.address=resourcemanager:8032 \
      --conf spark.yarn.resourcemanager.scheduler.address=resourcemanager:8030 \
      --conf spark.hadoop.dfs.client.use.datanode.hostname=true \
      --py-files hdfs://namenode:8020/user/jenkins/application_spark/dependencies.zip \
      hdfs://namenode:8020/user/jenkins/application_spark/app.py \
      --env TEST \
      --step {task_id} \
      --task-id {task_id} \
      --query_mapping {query_mapping} \
      --datamart {datamart} \
      --query_path {query_path} \
      --table_schema {table_schema} \
      --table_name {table_name} \
      --repartition {repartition} \
      --partition_by {partition_by} \
      --bucket_by {bucket_by} \
      --num_buckets {num_buckets} \
      --location {location} \
      --do_truncate_table {do_truncate_table} \
      --do_drop_table {do_drop_table} \
      --do_msck_repair_table {do_msck_repair_table} \
      --temp_view_name {temp_view_name} \
      --cache_df {cache_df}
    \"\"\"
    return BashOperator(
        task_id=f'spark_{task_id}',
        bash_command=bash_command,
        dag=dag,
    )

start_task = DummyOperator(task_id='start_pipeline', dag=dag)
end_task = DummyOperator(task_id='end_pipeline', dag=dag)
    
${tasksContent.toString()}

${dependenciesContent}
"""
                
                // Write DAG file
                writeFile file: "/opt/airflow/dags/${dagId}.py", text: dagContent
                echo "DAG файл создан: /opt/airflow/dags/${dagId}.py"
            }
            
            echo "\n=== ИТОГИ ГЕНЕРАЦИИ DAG ==="
            echo "Всего создано DAG файлов: ${allConfigs.size()}"
        }
    }
}

       

        
        stage('Проверка соединения') {
            steps {
                script {
                    echo "Проверка namenode (WebHDFS):"
                    sh 'curl -v "http://namenode:9870/webhdfs/v1/?op=LISTSTATUS&user.name=jenkins"'
                    
                    echo "Проверка доступности datanode:"
                    sh '''
                        # Проверяем доступность порта 9864
                        if curl -f -s -o /dev/null -m 5 "http://datanode:9864"; then
                            echo "✅ Datanode доступен на порту 9864"
                            # Альтернативная проверка состояния datanode
                            echo "Попытка получить информацию о блоках:"
                            curl -v "http://namenode:9870/webhdfs/v1/?op=GETCONTENTSUMMARY&user.name=jenkins"
                        else
                            echo "❌ Datanode недоступен на порту 9864"
                            exit 1
                        fi
                    '''
                    
                    echo "Проверка связи между контейнерами:"
                    sh '''
                        # Проверяем разрешение имени
                        echo "IP адрес datanode: $(getent hosts datanode || echo 'Не удалось разрешить имя')"
                        
                        # Альтернативная проверка доступности порта через curl
                        if curl -f -s -o /dev/null -m 5 "http://datanode:9864"; then
                            echo "✅ Соединение с datanode:9864 успешно"
                        else
                            echo "⚠️ Не удалось установить соединение с datanode:9864"
                            echo "Проверка через curl:"
                            curl -v "http://datanode:9864" || true
                            exit 1
                        fi
                    '''
                }
            }
        }

        stage('Загрузка через WebHDFS') {
            steps {
                script {
                    def WEBHDFS_NAMENODE = "http://namenode:9870/webhdfs/v1"
                    def HDFS_USER = "jenkins"
                    def HDFS_BASE_PATH = "${env.HDFS_SQL_DIR}"

                    // Создание корневой директории
                    echo "Создание корневой директории ${HDFS_BASE_PATH}..."
                    sh """
                        curl -f -s -X PUT \
                        '${WEBHDFS_NAMENODE}${HDFS_BASE_PATH}?op=MKDIRS&user.name=${HDFS_USER}'
                    """

                    // Поиск и загрузка всех SQL-файлов с сохранением структуры каталогов
                    dir('sql') {
                        def files = findFiles(glob: '**/*.sql')  // Рекурсивный поиск всех SQL-файлов
                        
                        if (files.size() == 0) {
                            echo "ℹ️ SQL-файлы не найдены"
                            return
                        }

                        files.each { file ->
                            try {
                                // Полный относительный путь от корня папки sql
                                def relativePath = file.path
                                def hdfsFilePath = "${HDFS_BASE_PATH}/${relativePath}"
                                def hdfsDirPath = hdfsFilePath.substring(0, hdfsFilePath.lastIndexOf('/'))
                                
                                // Создаем поддиректории если нужно
                                if (hdfsDirPath.length() > HDFS_BASE_PATH.length()) {
                                    echo "Создание директории ${hdfsDirPath}"
                                    sh """
                                        curl -f -s -X PUT \
                                        '${WEBHDFS_NAMENODE}${hdfsDirPath}?op=MKDIRS&user.name=${HDFS_USER}'
                                    """
                                }

                                echo "⏫ Загрузка ${file.path} в ${hdfsFilePath}"

                                // Получение redirect URL и загрузка файла
                                sh """
                                    redirect_url=\$(curl -f -s -i -X PUT \
                                    '${WEBHDFS_NAMENODE}${hdfsFilePath}?op=CREATE&user.name=${HDFS_USER}&overwrite=true' \
                                    | grep -i '^Location:' | cut -d' ' -f2 | tr -d '\r\n')
                                    
                                    curl -f -s -X PUT -T "${file.path}" "\$redirect_url"
                                """
                                echo "✅ Успешно загружен: ${file.path}"
                            } catch (Exception e) {
                                echo "⚠️ Ошибка загрузки ${file.path}: ${e.getMessage()}"
                            }
                        }
                    }
                }
            }
        }       
    }
}
