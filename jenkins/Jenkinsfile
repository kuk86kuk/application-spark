pipeline {
    agent any
    
    parameters {
        string(
            name: 'GIT_REPO_URL',
            defaultValue: 'https://github.com/kuk86kuk/DataMart_transaction',
            description: 'URL Git-репозитория с конфигами'
        )
        string(
            name: 'CONFIG_DIR',
            defaultValue: './wf',
            description: 'Путь к директории с YAML-конфигами'
        )
        string(
            name: 'DATAMARTS',
            defaultValue: 'DataMart_transaction1',
            description: 'Название витрины'
        )
    }

    environment {
        HADOOP_NAMENODE = "exec"
        AIRFLOW_DAGS_DIR = "/opt/airflow/dags"
        HDFS_SQL_DIR = "/datamarts/${params.DATAMARTS}"
        REPO_NAME = "${env.JOB_NAME}"

        AIRFLOW_API = "http://airflow-airflow-1:8080/api/v1"
        AIRFLOW_USER = "admin"
        AIRFLOW_PASS = "admin"
    }

    stages {
        stage('Получение кода') {
            steps {
                checkout([
                    $class: 'GitSCM',
                    branches: [[name: '*/main']],
                    extensions: [],
                    userRemoteConfigs: [[
                        url: "${params.GIT_REPO_URL}",
                        credentialsId: 'github-ssh-key'
                        
                    ]]
                ])
                sh 'ls -R'
            }
        }

       stage('Чтение YAML конфигов и генерация DAG') {
    steps {
        script {
            echo "=== НАЧАЛО ОБРАБОТКИ YAML КОНФИГОВ ==="
            
            // 1. Обработка common.yaml
            echo "Поиск common.yaml в ${params.CONFIG_DIR}"
            def commonConfig = [:]
            try {
                commonConfig = readYaml file: "${params.CONFIG_DIR}/common.yaml"
                echo "Успешно прочитан common.yaml:"
                echo "Версия: ${commonConfig.version ?: 'не указана'}"
                echo "Другие параметры: ${commonConfig.findAll { it.key != 'version' }}"
            } catch (Exception e) {
                echo "⚠️ ВНИМАНИЕ: Не удалось прочитать common.yaml"
                echo "Ошибка: ${e.getMessage()}"
                echo "Продолжаем работу без common.yaml"
            }

            // 2. Поиск других YAML-файлов
            echo "Поиск других YAML-файлов в ${params.CONFIG_DIR}"
            def findCmd = "find ${params.CONFIG_DIR} -type f \\( -name '*.yaml' -o -name '*.yml' \\) ! -name 'common.yaml' 2>/dev/null || echo ''"
            echo "Выполняем команду: ${findCmd}"
            
            def otherFiles = sh(script: findCmd, returnStdout: true).trim().split('\n')
            echo "Найдено файлов: ${otherFiles.size()}"
            
            def allConfigs = [:]
            otherFiles.eachWithIndex { filePath, index ->
                echo "\n=== Обработка файла ${index + 1}/${otherFiles.size()}: ${filePath} ==="
                // Чтение файла
                def config = readYaml file: filePath
                echo "Файл прочитан успешно"
                
                // Объединение с commonConfig
                def mergedConfig = commonConfig + config
                def configName = filePath.tokenize('/').last().replace('.yaml', '').replace('.yml', '')
                allConfigs[configName] = mergedConfig
            }

            // Сохранение параметров для DAG
            env.SCHEDULE_INTERVAL = allConfigs.flow?.dependencies?.find { it.name == "full_load" }?.schedule ?: "@daily"
            env.START_DATE = "2024-01-01"
            
            echo "\n=== ИТОГИ ОБРАБОТКИ КОНФИГОВ ==="
            echo "Всего успешно обработано конфигов: ${allConfigs.size()}"
            echo "Список конфигов: ${allConfigs.keySet().join(', ')}"

            echo "=== НАЧАЛО ГЕНЕРАЦИИ DAG ФАЙЛОВ ==="
            
           
            // Process each config to create a DAG
            allConfigs.each { configName, config ->
                def dagId = "${params.DATAMARTS}_${configName}"
                echo "Создание DAG: ${dagId}"
                
                // Извлекаем параметры из конфига
                def scheduleInterval = config.flow?.dependencies?.find { it.name == "full_load" }?.schedule ?: "@once"
                def startDate = config.flow?.dependencies?.find { it.name == "full_load" }?.start_date ?: "2023-01-01"
                
                // Генерируем задачи из конфига
                def tasksContent = ""
                def tasksDependencies = ""
                if (config.flow?.dependencies?.find { it.name == "full_load" }?.tasks) {
                    def tasks = config.flow.dependencies.find { it.name == "full_load" }.tasks
                    tasks.each { task ->
                        tasksContent += "    ${task} = DummyOperator(task_id='${task}')\n"
                    }
                    tasksDependencies = "    start >> " + tasks.join(" >> ") + " >> end"
                }
                
                // Generate DAG content
                def dagContent = """from airflow import DAG
from airflow.operators.dummy import DummyOperator
from datetime import datetime

with DAG(
    "${dagId}",
    schedule_interval="${scheduleInterval}",
    start_date=datetime.strptime("${startDate}", "%Y-%m-%d"),
    catchup=False,
    tags=["jenkins", "${params.DATAMARTS}"],
    params=${config}
) as dag:
    start = DummyOperator(task_id="start")
    end = DummyOperator(task_id="end")
    
${tasksContent}
${tasksDependencies ?: '    start >> end'}
"""
                
                // Write DAG file
                writeFile file: "/var/jenkins_dags/${dagId}.py", text: dagContent
                echo "DAG файл создан: /var/jenkins_dags/${dagId}.py"
            }
            
            echo "\n=== ИТОГИ ГЕНЕРАЦИИ DAG ==="
            echo "Всего создано DAG файлов: ${allConfigs.size()}"
        }
    }
}

       

        
        stage('Проверка соединения') {
            steps {
                script {
                    echo "Проверка namenode (WebHDFS):"
                    sh 'curl -v "http://namenode:9870/webhdfs/v1/?op=LISTSTATUS&user.name=jenkins"'
                    
                    echo "Проверка доступности datanode:"
                    sh '''
                        # Проверяем доступность порта 9864
                        if curl -f -s -o /dev/null -m 5 "http://datanode:9864"; then
                            echo "✅ Datanode доступен на порту 9864"
                            # Альтернативная проверка состояния datanode
                            echo "Попытка получить информацию о блоках:"
                            curl -v "http://namenode:9870/webhdfs/v1/?op=GETCONTENTSUMMARY&user.name=jenkins"
                        else
                            echo "❌ Datanode недоступен на порту 9864"
                            exit 1
                        fi
                    '''
                    
                    echo "Проверка связи между контейнерами:"
                    sh '''
                        # Проверяем разрешение имени
                        echo "IP адрес datanode: $(getent hosts datanode || echo 'Не удалось разрешить имя')"
                        
                        # Альтернативная проверка доступности порта через curl
                        if curl -f -s -o /dev/null -m 5 "http://datanode:9864"; then
                            echo "✅ Соединение с datanode:9864 успешно"
                        else
                            echo "⚠️ Не удалось установить соединение с datanode:9864"
                            echo "Проверка через curl:"
                            curl -v "http://datanode:9864" || true
                            exit 1
                        fi
                    '''
                }
            }
        }

        stage('Загрузка через WebHDFS') {
            steps {
                script {
                    def WEBHDFS_NAMENODE = "http://namenode:9870/webhdfs/v1"
                    def HDFS_USER = "jenkins"
                    def HDFS_BASE_PATH = "${env.HDFS_SQL_DIR}"

                    // Создание корневой директории
                    echo "Создание корневой директории ${HDFS_BASE_PATH}..."
                    sh """
                        curl -f -s -X PUT \
                        '${WEBHDFS_NAMENODE}${HDFS_BASE_PATH}?op=MKDIRS&user.name=${HDFS_USER}'
                    """

                    // Поиск и загрузка всех SQL-файлов с сохранением структуры каталогов
                    dir('sql') {
                        def files = findFiles(glob: '**/*.sql')  // Рекурсивный поиск всех SQL-файлов
                        
                        if (files.size() == 0) {
                            echo "ℹ️ SQL-файлы не найдены"
                            return
                        }

                        files.each { file ->
                            try {
                                // Полный относительный путь от корня папки sql
                                def relativePath = file.path
                                def hdfsFilePath = "${HDFS_BASE_PATH}/${relativePath}"
                                def hdfsDirPath = hdfsFilePath.substring(0, hdfsFilePath.lastIndexOf('/'))
                                
                                // Создаем поддиректории если нужно
                                if (hdfsDirPath.length() > HDFS_BASE_PATH.length()) {
                                    echo "Создание директории ${hdfsDirPath}"
                                    sh """
                                        curl -f -s -X PUT \
                                        '${WEBHDFS_NAMENODE}${hdfsDirPath}?op=MKDIRS&user.name=${HDFS_USER}'
                                    """
                                }

                                echo "⏫ Загрузка ${file.path} в ${hdfsFilePath}"

                                // Получение redirect URL и загрузка файла
                                sh """
                                    redirect_url=\$(curl -f -s -i -X PUT \
                                    '${WEBHDFS_NAMENODE}${hdfsFilePath}?op=CREATE&user.name=${HDFS_USER}&overwrite=true' \
                                    | grep -i '^Location:' | cut -d' ' -f2 | tr -d '\r\n')
                                    
                                    curl -f -s -X PUT -T "${file.path}" "\$redirect_url"
                                """
                                echo "✅ Успешно загружен: ${file.path}"
                            } catch (Exception e) {
                                echo "⚠️ Ошибка загрузки ${file.path}: ${e.getMessage()}"
                            }
                        }
                    }
                }
            }
        }       
    }
}
